---
title: "Project 1"
author: "Jiayu Ni(jn2585)"
date: "Sep 15, 2017"
output: html_document
---

#Step1: 
##check and install needed packages. Load the libraries.

####In the first part, require all package used below.
```{r setup,include=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}
if (Sys.getenv("JAVA_HOME")!="")
  Sys.setenv(JAVA_HOME="")
library(rJava)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(tidytext)
library(SnowballC)
library(wordnet)
library(ggplot2)
library(sentimentr)
library(qdap)
library(NLP)
library(openNLP)
library(topicmodels)
library(syuzhet)
library(beeswarm)
library(factoextra)
library(cluster)   
library(fpc) 
# The notebook was prepared with the following version.
print(R.version)
```

#Step2:
##Data harvest and process 

####In this part, I read all inauguration, process data and remove all unnecessary words including setting stop words, punctuation and numbers.


```{r}
folder.path="../data/InauguralSpeeches/"
speech.list=list.files(path=folder.path,pattern = "*.txt")
prez.out=substr(speech.list, 6, nchar(speech.list)-4)


ff.all <- Corpus(DirSource(folder.path))
ff.all <- tm_map(ff.all,stripWhitespace)
ff.all <- tm_map(ff.all,content_transformer(tolower))
#set up stop words
myStopwords <- c("can","say","one","way","use","also","howev","tell","will","much","need","take","tend","even","like","particular","rather","said","get","well","make","ask","come","end","first","two","help","often","may","might","see","someth","thing","point","post","look","right","now","think","'ve","'re","anoth","put","set","new","good","want","sure","kind","larg","yes","day","quit","sinc","attempt","lack","seen","awar","littl","ever","moreov","though","found","abl","enough","far","earli","away","achiev","last","never","brief","bit","entir","lot","must","shall")
ff.all <- tm_map(ff.all,removeWords,stopwords("english"))
ff.all <- tm_map(ff.all,removeWords,myStopwords)
ff.all <- tm_map(ff.all,stemDocument)
ff.all <- tm_map(ff.all,removeWords,character(0))###
ff.all <- tm_map(ff.all,removePunctuation)
ff.all <- tm_map(ff.all,removeNumbers)
dtm2 <- DocumentTermMatrix(ff.all)

```

##Constructing dates matrix

####Since I would like to see the change of inauguration with time passing by, this chunk I mainly deal with dates data of inauguration and make it a tidy data frame, so that It could be merge with the word matrix and sentence matrix.
 
```{r,warning=FALSE}
dates<- read.table("../data/InauguationDates.txt",header = T,sep="\t")
for( i in 1:ncol(dates)){dates[,i]<- dates[,i] %>% as.character()}  
choose_year <- function(x){substr(x,nchar(x)-3,nchar(x))} #set a function to choose year rather than day/month/year
year_data<- sapply(dates[,2:5],choose_year)


# select white space and . and replace with"" to have the same format with prez.out
test<- gregexpr("\\s|[[:punct:]]",dates$PRESIDENT)
regmatches(dates$PRESIDENT,test) <- ""
dates <- cbind(dates[,1],year_data) %>%data.frame()
for(i in 2:5){dates[,i] <- as.numeric(as.character(dates[,i]))}
dates[,1] <- dates[,1]%>% as.character()
dates[dates[,1]=="GroverCleveland",1]<-c("GroverCleveland-I","GroverCleveland-II")# Since president GroverCleveland is special, I set it manually here. 

# set a function to select president, corresponding year, and term. 
dates_function <- function(x){
  term_value<- sum(!is.na(x[,2:5]))
  answer <- NULL
  for(i in 1:term_value){
    answer1 <- c(x[,1],term=i,x[,1+i])
    answer <- rbind(answer,answer1)
  }
  return(answer)
}
# use the function above to selecet data
answer <- NULL
for(i in 1:dim(dates)[1]){
  answer <- rbind(answer,dates_function(dates[i,]))
}
colnames(answer) <- c("File","term","year")# assign colnames to the data frame.
answer <- data.frame(answer)

for(i in 2:3){
  answer[,i] <- as.numeric(as.character(answer[,i]))}
index<- which(is.na(answer[,3])) #since president GroverCleverland is special, I deal with it manually.
answer[index,3] <- dates[dates[,1]=="GroverCleveland-II",3]
answer[index,2] <- 2
colnames(answer) <- c("File",colnames(answer)[2:3])
dates_prez <- answer
head(dates_prez)
```

#Step3: Term analysis/Topic analysis

##topic modeling

####In this chunk, I generate document term matrix and use LDA method to do topic modeling. Illustrate 15 topics and topic terms.In this case, k=15, means 15 topics.

```{r}
dtm <- DocumentTermMatrix(ff.all)

freq <- colSums(as.matrix(dtm))
ord <- order(freq,decreasing = T)
row.names(dtm) <- speech.list
rowTotals <- apply(dtm,1,sum)


burnin <- 4000
iter <- 2000
thin <- 500
seed <- 2003
nstart <- 5
best <- TRUE

k <- 15

ldaout <- LDA(dtm,k,method = "Gibbs",control=list(seed=seed,best=best,burnin=burnin,iter=iter,thin=thin))

ldaout.topics <- as.matrix(topics(ldaout))
table(c(1:k,ldaout.topics))


topicProbabilities <- as.data.frame(ldaout@gamma)

terms.beta=ldaout@beta #select term out
terms.beta=scale(terms.beta)


ldaOut.terms <- as.matrix(terms(ldaout,20))
ldaOut.terms

```
##Hierarchal Term Clustering

####In this part, I continue to process data and cluster the most used terms into 7 group 
```{r}
dtmss <- removeSparseTerms(dtm, 0.2)
d <- dist(t(dtmss), method="euclidian")   
fit <- hclust(d=d, method="ward.D")   
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=7)  # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=7, border="red") 
```
##K means cluster

### In this part, I use k means clusters to cluster the terms into 3 groups and illustrate it clearly.

```{r}
d <- dist(t(dtmss), method="euclidian")   
kfit <- kmeans(d, 3)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0) 

```
##term frequence

###In this part, I would like to show the frequence of the most used term in all 58 inauguration. Since I have already set stop words above, terms such like "must", "shall" are not listed, this could show the more meaningful terms.
```{r, warning=FALSE}

freq <- sort(colSums(as.matrix(dtm)))

wf <- data.frame(word=names(freq),freq=freq)

freq2 <- colSums(as.matrix(dtm))

p <- ggplot(subset(tail(wf,20),freq>50),aes(word,freq))
p <- p+geom_bar(stat = "identity")
p <- p+theme(axis.text.x = element_text(angle = 45,hjust = 1,size = 10))
p

```
##word cloud

###In this part, I mainly use two method to show the word cloud of the most frequently used terms. 
```{r,warning=FALSE}
wordcloud(names(freq),freq,min.freq = 100,scale = c(5,.1),colors=brewer.pal(6,"Dark2"))
```

#Step4: sentence analysis

###In this part, I load all the sentences and construct emotions matrix with function to prepare for sentence analysis
```{r,warning=FALSE}
speech.sentance<- lapply(speech.list,function(x)readLines(paste("../data/InauguralSpeeches/",x,sep = "")))

sentence.list=NULL
for(i in 1:length(speech.sentance)){
  sentences=sent_detect(speech.sentance[[i]],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    #Count the word in each sentences
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    term =as.numeric(substr(prez.out[i],nchar(prez.out[i]),nchar(prez.out[i])))
    File=as.character(substr(prez.out,1,nchar(prez.out)-2)[i])
    sentence.list=rbind(sentence.list, 
            cbind(sentences=as.character(sentences),
                  File,
                  term,
                  word.count,
                  emotions,
                  sent.id=1:length(sentences)
                  )
    )
  }
}

sentence.list<- data.frame(sentence.list)
sentence.list=sentence.list%>%filter(!is.na(word.count))
sentence.list <- data.frame(sentence.list)
name_inaug<- colnames(sentence.list)

sentence.list_part1 <- sentence.list[,1:2]
sentence.list_part2 <- NULL
for (k in 3:15){
  sentence.list_part2 <- cbind(sentence.list_part2,as.numeric(as.character(sentence.list[,k])))
}
sentence.list <-cbind(sentence.list_part1,sentence.list_part2)
colnames(sentence.list) <- name_inaug
```

####In this chunk, I merge the sentence matrix with the date matrix constructed above through names of president and terms of each president.

```{r}
sentence.list<- merge(sentence.list,dates_prez,by.x = c("File","term"),by.y = c("File","term"))
colnames(sentence.list)<-c(colnames(sentence.list)[1:length(colnames(sentence.list))-1],"year")
head(sentence.list,2)
```

####In this chunk, I plot the total length of sentences of every inauguration to see if there is any trend existing.

```{r,warning=FALSE}
par(mfrow=c(1,2))

sentence.list_term1 <- filter(sentence.list,term==1)

sentence.list_term2 <- filter(sentence.list,term==2)

term1_data<- group_by(sentence.list_term1,year)
timeline1<- summarise(term1_data,
          totalwords=sum(word.count),
          totalsentenca=length(sent.id)
          )

ggplot(data =timeline1 )+
  geom_line(mapping = aes(x=year,y=totalwords))+
  labs(title="term1 totalwords")

term2_data<- group_by(sentence.list_term2,year)
timeline2<- summarise(term2_data,
          totalwords=sum(word.count),
          totalsentenca=length(sent.id)
          )

ggplot(data =timeline2 )+
  geom_line(mapping = aes(x=year,y=totalwords))+
  labs(title="term2 totalwords")

```


####Since Donald Trump has been so popular recently, In this chunk, I check the shortest and longest sentences that Donald Trump said.
```{r}
sentence_Donald<- filter(sentence.list,File=="DonaldJTrump")
arrange(sentence_Donald,desc(word.count))[1:3,"sentences"]
arrange(sentence_Donald,word.count)[1:10,"sentences"]
```


```{r,include=FALSE}
f.plotsent.len=function(In.list, InFile, InTerm, President){
  
  col.use=c("lightgray", "red2", "darkgoldenrod1", 
            "chartreuse3", "blueviolet",
            "darkgoldenrod2", "dodgerblue3", 
            "darkgoldenrod1", "darkgoldenrod1",
            "black", "darkgoldenrod2")
  
  In.list$topemotion=apply(select(In.list,anger:positive),1, which.max)
  In.list$topemotion.v=apply(select(In.list,anger:positive), 1, max)
  In.list$topemotion[In.list$topemotion.v<0.05]=0
  In.list$topemotion=In.list$topemotion+1
  
  temp=In.list$topemotion.v
  In.list$topemotion.v[temp<0.05]=1
  
  df=In.list%>%filter(File==InFile,term==InTerm)%>%
    select(sent.id, word.count, topemotion,topemotion.v)
  
  ptcol.use=alpha(col.use[df$topemotion],sqrt(sqrt(df$topemotion.v)))
  
  plot(df$sent.id, df$word.count, 
       col=ptcol.use,
       type="h", #ylim=c(-10, max(In.list$word.count)),
       main=President)
}
```

## Relationship between Sentence length and emotions
####In this chunk, f.plotsent.len is a function defined above and it mainly choose the most strong emotion in each sentence and assign specific color to that sentence.
```{r}
par(mfrow=c(3,1), mar=c(1,0,2,0), bty="n", xaxt="n", yaxt="n", font.main=1)

f.plotsent.len(In.list=sentence.list, InFile="DonaldJTrump",InTerm=1, President="Donald Trump")

f.plotsent.len(In.list=sentence.list, InFile="BarackObama",  InTerm=1, President="Barack Obama")

f.plotsent.len(In.list=sentence.list, InFile="GeorgeWBush", InTerm=1, President="George W. Bush")
```


####The most emotionally charged sentences of Barack Obama and Donald Trump
```{r}
print("Barack Obama")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="BarackObama", term==1, word.count>=5)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])


print("Donald Trump")
speech.df=tbl_df(sentence.list)%>%
  filter(File=="DonaldJTrump", term==1,word.count>=5)%>%
  select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
```
## comparation of emotions

####In this chunk, I compare the emotions between all inaugural speeches, speeche of Donald Trump and speech of Obama 

```{r}
#The distribution of emotions in all sentences
par(mfrow=c(3,1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
barplot(height = emo.means[order(emo.means)],horiz = TRUE,col=1:8,main="Inaugural Speeches",names=names(emo.means),legend = names(emo.means),args.legend = list(x="bottomright",cex=0.9))

emo.means=colMeans(select(sentence_Donald, anger:trust)>0.01)
barplot(height = emo.means[order(emo.means)],horiz = TRUE,col=1:8,main="Donald Trump",names=names(emo.means),legend = names(emo.means),args.legend = list(x="bottomright",cex=0.9))

sentence_Obama<- filter(sentence.list,File=="BarackObama")
emo.means=colMeans(select(sentence_Obama, anger:trust)>0.01)
barplot(height = emo.means[order(emo.means)],horiz = TRUE,col=1:8,main="Barack Obama",names=names(emo.means),legend = names(emo.means),args.legend = list(x="bottomright",cex=0.9))

```




